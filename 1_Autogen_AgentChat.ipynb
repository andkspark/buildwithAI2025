{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "해당 노트는 Hello World & Build with AI in Incheon 2025의 진행을 위하여 제작되었습니다.  \n",
        "제작 : 박광석(모두의연구소, https://www.linkedin.com/in/andkspark)\n",
        "\n",
        "해당 노트는 Autogen을 처음 접하시는 분들을 위한 튜토리얼입니다.  \n",
        "참고 : https://microsoft.github.io/autogen/stable//user-guide/agentchat-user-guide/index.html"
      ],
      "metadata": {
        "id": "5OJXxoCxKX-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0 : 설치와 준비  \n",
        "Autogen 설치 및 Gemini API 키를 등록하도록 합니다."
      ],
      "metadata": {
        "id": "6Ok97bSjeJ2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNFGl5FxrkG8",
        "outputId": "6cb9f182-2b69-46d3-efeb-968e5e4fba0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen\n",
            "  Downloading autogen-0.8.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting pyautogen==0.8.5 (from autogen)\n",
            "  Downloading pyautogen-0.8.5-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.5->autogen) (4.9.0)\n",
            "Collecting asyncer==0.0.8 (from pyautogen==0.8.5->autogen)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting diskcache (from pyautogen==0.8.5->autogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from pyautogen==0.8.5->autogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.5->autogen) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.5->autogen) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.5->autogen) (2.11.1)\n",
            "Collecting python-dotenv (from pyautogen==0.8.5->autogen)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.5->autogen) (3.0.0)\n",
            "Collecting tiktoken (from pyautogen==0.8.5->autogen)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen==0.8.5->autogen) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen==0.8.5->autogen) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen==0.8.5->autogen) (4.13.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->pyautogen==0.8.5->autogen) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->pyautogen==0.8.5->autogen) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.28.1->pyautogen==0.8.5->autogen) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.5->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.5->autogen) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.5->autogen) (0.4.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen==0.8.5->autogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen==0.8.5->autogen) (2.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->pyautogen==0.8.5->autogen) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->pyautogen==0.8.5->autogen) (3.4.1)\n",
            "Downloading autogen-0.8.5-py3-none-any.whl (12 kB)\n",
            "Downloading pyautogen-0.8.5-py3-none-any.whl (730 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.4/730.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, diskcache, tiktoken, docker, asyncer, pyautogen, autogen\n",
            "Successfully installed asyncer-0.0.8 autogen-0.8.5 diskcache-5.6.3 docker-7.1.0 pyautogen-0.8.5 python-dotenv-1.1.0 tiktoken-0.9.0\n",
            "Collecting autogen-agentchat\n",
            "  Downloading autogen_agentchat-0.5.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting autogen-core==0.5.1 (from autogen-agentchat)\n",
            "  Downloading autogen_core-0.5.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting jsonref~=1.1.0 (from autogen-core==0.5.1->autogen-agentchat)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (1.31.1)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (11.1.0)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (2.11.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (4.13.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-agentchat) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-agentchat) (8.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-agentchat) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-agentchat) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-agentchat) (0.4.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-agentchat) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-agentchat) (3.21.0)\n",
            "Downloading autogen_agentchat-0.5.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogen_core-0.5.1-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Installing collected packages: jsonref, autogen-core, autogen-agentchat\n",
            "Successfully installed autogen-agentchat-0.5.1 autogen-core-0.5.1 jsonref-1.1.0\n",
            "Collecting autogen-ext[openai]\n",
            "  Downloading autogen_ext-0.5.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: autogen-core==0.5.1 in /usr/local/lib/python3.11/dist-packages (from autogen-ext[openai]) (0.5.1)\n",
            "Collecting aiofiles (from autogen-ext[openai])\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: openai>=1.66.5 in /usr/local/lib/python3.11/dist-packages (from autogen-ext[openai]) (1.70.0)\n",
            "Requirement already satisfied: tiktoken>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from autogen-ext[openai]) (0.9.0)\n",
            "Requirement already satisfied: jsonref~=1.1.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext[openai]) (1.1.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext[openai]) (1.31.1)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext[openai]) (11.1.0)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext[openai]) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext[openai]) (2.11.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext[openai]) (4.13.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=1.66.5->autogen-ext[openai]) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.66.5->autogen-ext[openai]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.66.5->autogen-ext[openai]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.66.5->autogen-ext[openai]) (0.14.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-ext[openai]) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-ext[openai]) (8.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-ext[openai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-ext[openai]) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-ext[openai]) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai]) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai]) (2.3.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-ext[openai]) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-ext[openai]) (3.21.0)\n",
            "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading autogen_ext-0.5.1-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.9/259.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: aiofiles, autogen-ext\n",
            "Successfully installed aiofiles-24.1.0 autogen-ext-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install autogen\n",
        "!pip install -U \"autogen-agentchat\"\n",
        "!pip install \"autogen-ext[openai]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "HeIsGZ_UzanI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.conditions import TextMentionTermination\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "from autogen_agentchat.messages import TextMessage"
      ],
      "metadata": {
        "id": "lNNZVFMTyBuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API KEY는 관리에 유의하시기 바랍니다!"
      ],
      "metadata": {
        "id": "XbdyHqwiUej0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY ="
      ],
      "metadata": {
        "id": "qXMrlMKRyv7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoGen은 복잡한 워크플로우를 가능하게 하는 오픈 소스 프레임워크로, 에이전트(agent)라는 개념을 중심으로 구성되어 있습니다!  \n",
        "에이전트는 메시지를 주고받으며, 대형 언어 모델(LLM)이나 코드 실행기, 인간의 입력 등 다양한 구성 요소를 통해 응답을 생성할 수 있고, 외부 함수나 도구를 호출하여 특정 작업을 수행할 수 있도록 지원합니다."
      ],
      "metadata": {
        "id": "35cLjIrwtZzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 : 에이전트 정의하기\n",
        "Autogen에서는 이름과 시스템 메세지, 그리고 model client 만으로\n",
        "매우 간편하게 에이전트를 정의할 수 있습니다!\n",
        "  \n",
        "model client는 \"에이전트가 LLM(예: GPT-4)이랑 대화할 수 있게 해주는 연결 다리\" 입니다. 다양한 모델의 API간의 다른 모델 호출 방식을 Autogen 코드 내에서 통일된 방식으로 접근할 수 있게 해줍니다.  \n",
        "  \n",
        "실습 코드에서 사용하는 OpenAIChatCompletionClient 는 OpenAI의 모델, 혹은 OpenAI API와 호환을 제공하는 모델 (예 : Gemini) 을 사용할 수 있습니다.  \n",
        "그 외에도 Azure OpenAI models 을 지원하는 AzureOpenAIChatCompletionClient, Claude 등의 Anthropic 모델 /  Ollema 을 지원하는 AnthropicChatCompletionClient / OllamaChatCompletionClient 가 있으며, semanticp-kernel 을 이용해 mistral, aws, huggingface의 모델을 사용할 수 있는 SKChatCompletionAdapter 도 지원합니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YcEMf2QZwzlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemini 같이 OpenAI 모델이 아니면서 OpenAI API를 지원하는 경우 base_url 이 필요합니다\n",
        "model_client=OpenAIChatCompletionClient(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    api_key = GOOGLE_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "    )"
      ],
      "metadata": {
        "id": "PnUdAhLxZLnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"넌 정말 똑똑한 친구야!\",\n",
        ")"
      ],
      "metadata": {
        "id": "5aUoXSoSZJbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 : 메세지 보내기"
      ],
      "metadata": {
        "id": "_QNgs10SpGws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autogen은 에이전트를 여럿 두고 대화를 주고받는 형태로 시스템을 구성합니다.  물론 유저 또한 에이전트와 대화를 주고받을 수 있습니다!  \n",
        "단일 에이전트와 Message를 통하여 대화를 주고받아보도록 하겠습니다.\n",
        "\n",
        "면밀한 관찰을 위해 해당 실습에서는 autogen을 비동기식 (async) 으로 사용합니다.비동기식으로 autogen을 사용할 경우, 지연상황을 발생시키지 않기 위해 CancellationToken을 함께 사용합니다.\n",
        "  \n",
        "CancellationToken은 autogen.core 라이브러리에 정의되어 있습니다."
      ],
      "metadata": {
        "id": "5d2ufiig2ceb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_core import CancellationToken"
      ],
      "metadata": {
        "id": "iaiCqRBpZmH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "메세지에는 source가 함께 표기됩니다. 이후 진행해볼 에이전트간의 대화에서는 메세지를 발신한 에이전트의 정보가 표시됩니다."
      ],
      "metadata": {
        "id": "h7aoncOI28Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_message = TextMessage(content=\"Hello World! Let's build with AI\", source=\"User\")\n",
        "text_message"
      ],
      "metadata": {
        "id": "7iVtJE2QDQch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586a863e-22a5-402c-9165-1bd235f49fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextMessage(source='User', models_usage=None, metadata={}, content=\"Hello World! Let's build with AI\", type='TextMessage')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정의한 메세지로 에이전트와 대화해보겠습니다.\n",
        "on_messages 메소드를 사용하여 정의한 메세지를 주입합니다."
      ],
      "metadata": {
        "id": "tl1rQSaK4Ema"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = await agent.on_messages(\n",
        "    [text_message], cancellation_token=CancellationToken()\n",
        ")\n",
        "response.chat_message"
      ],
      "metadata": {
        "id": "s2IBprR6aWY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d29948-ecc8-4970-fa50-3c483066fb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=187, completion_tokens=334), metadata={}, content=\"Okay, I'm ready to build! To start, can you give me a little more information about what you have in mind?  For example:\\n\\n*   **What kind of project are you interested in?**  (e.g., a web application, a game, a poem, a machine learning model, an automation script, etc.)\\n*   **What are your goals for this project?** (e.g., learn a new skill, solve a specific problem, create something entertaining, etc.)\\n*   **What are your existing skills and experience?** (e.g., are you a beginner, intermediate, or expert programmer? Do you have experience with specific AI tools or libraries?)\\n*   **Do you have any specific ideas or constraints?** (e.g., you want to use a particular technology, you have a limited budget, etc.)\\n\\nThe more information you provide, the better I can help you get started.  I can assist with things like:\\n\\n*   **Brainstorming ideas:**  If you're not sure what to build, I can suggest some possibilities based on your interests and skill level.\\n*   **Planning your project:**  I can help you break down your project into smaller, more manageable tasks.\\n*   **Generating code:**  I can write code snippets or even entire programs for you.\\n*   **Providing explanations:**  I can explain complex concepts and help you understand how things work.\\n*   **Debugging and troubleshooting:**  I can help you identify and fix errors in your code.\\n\\nSo, what shall we build together?\\n\", type='TextMessage')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "답변은 메세지 객체로 돌아오며, 멤버 변수 content에 접근하여 모델의 출력을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "GDUFOGTm4WfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"<USER> : \", text_message.content)\n",
        "print(\"<ASSISTANT> : \", response.chat_message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhUUA4ZZhHwi",
        "outputId": "d390f32f-6ee5-4e93-b8a4-9fc968c0b4bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<USER> :  Hello World! Let's build with AI\n",
            "<ASSISTANT> :  Okay, I'm ready to build! To start, can you give me a little more information about what you have in mind?  For example:\n",
            "\n",
            "*   **What kind of project are you interested in?**  (e.g., a web application, a game, a poem, a machine learning model, an automation script, etc.)\n",
            "*   **What are your goals for this project?** (e.g., learn a new skill, solve a specific problem, create something entertaining, etc.)\n",
            "*   **What are your existing skills and experience?** (e.g., are you a beginner, intermediate, or expert programmer? Do you have experience with specific AI tools or libraries?)\n",
            "*   **Do you have any specific ideas or constraints?** (e.g., you want to use a particular technology, you have a limited budget, etc.)\n",
            "\n",
            "The more information you provide, the better I can help you get started.  I can assist with things like:\n",
            "\n",
            "*   **Brainstorming ideas:**  If you're not sure what to build, I can suggest some possibilities based on your interests and skill level.\n",
            "*   **Planning your project:**  I can help you break down your project into smaller, more manageable tasks.\n",
            "*   **Generating code:**  I can write code snippets or even entire programs for you.\n",
            "*   **Providing explanations:**  I can explain complex concepts and help you understand how things work.\n",
            "*   **Debugging and troubleshooting:**  I can help you identify and fix errors in your code.\n",
            "\n",
            "So, what shall we build together?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 : 툴 사용하기\n",
        "\n",
        "Agent는 tool을 사용하여 작업을 수행할 수 있습니다.  \n",
        "Autogen에서 내장형으로 graph RAG, Langchain, mcp, http 도구를 지원하며, 사용자가 tool을 함수로 정의하여 사용할 수도 있습니다.  \n",
        "  \n",
        "이 섹션에서는 사용자가 정의한 툴을 사용해보겠습니다."
      ],
      "metadata": {
        "id": "40gdlA4Lilgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_core.tools import FunctionTool"
      ],
      "metadata": {
        "id": "inGHu4gURrJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "async def calculate_expression_func(input_string : str) -> str:\n",
        "    match = re.search(r'(\\d+)\\s*([x*/+-])\\s*(\\d+)', input_string)\n",
        "    if match:\n",
        "        num1 = int(match.group(1))\n",
        "        operator = match.group(2)\n",
        "        num2 = int(match.group(3))\n",
        "\n",
        "        if operator == 'x' or operator == '*':\n",
        "            result = num1 * num2\n",
        "        elif operator == '/':\n",
        "            result = num1 / num2\n",
        "        elif operator == '+':\n",
        "            result = num1 + num2\n",
        "        elif operator == '-':\n",
        "            result = num1 - num2\n",
        "\n",
        "        result *= 100\n",
        "\n",
        "        return result\n",
        "\n",
        "    else:\n",
        "        return None\n",
        "        # 수식을 찾을 수 없는 경우도구 None 반환"
      ],
      "metadata": {
        "id": "Nw9wGubBInMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "연산 결과의 100배를 돌려주는 함수입니다..!  \n",
        "함수 정의 후 도구를 생성해줍니다"
      ],
      "metadata": {
        "id": "TQKxky8SXKDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tool that searches the web for information.\n",
        "calculate_tool = FunctionTool(calculate_expression_func, description = \"100배로 반환해주는 도구\")"
      ],
      "metadata": {
        "id": "_B5bMGmOEdhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tool을 생성하면 내부적으로 Schema를 자동으로 생성합니다. 반환은 string으로 수행합니다.  "
      ],
      "metadata": {
        "id": "bd40UDJ5XLGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_tool.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FKNghFkTko_",
        "outputId": "c6673b5f-c371-4a69-ed81-b51994942dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'calculate_expression_func',\n",
              " 'description': '100배로 반환해주는 도구',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'input_string': {'description': 'input_string',\n",
              "    'title': 'Input String',\n",
              "    'type': 'string'}},\n",
              "  'required': ['input_string'],\n",
              "  'additionalProperties': False},\n",
              " 'strict': False}"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "어시스턴트에 도구를 쥐어주겠습니다!  \n",
        "아래 정의한 것 처럼, [] 안에 사용할 도구를 넣어줍니다.\n",
        "\n",
        "Reflect_on_tool_use 는 모델이 별개로 추론한 결과를 도구 사용 결과에 반영할 것인가를 결정합니다."
      ],
      "metadata": {
        "id": "mXdYQIc4X6rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    model_client=model_client,\n",
        "    tools=[calculate_tool],\n",
        "#    reflect_on_tool_use  = True,\n",
        "\n",
        "    system_message=\"값을 잘 계산해줘 \",\n",
        ")"
      ],
      "metadata": {
        "id": "uLtQWPAOEPNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def assistant_run() -> None:\n",
        "    response = await agent.on_messages(\n",
        "        [TextMessage(content=\"3 * 4 가 뭐야?\", source=\"user\")],\n",
        "        cancellation_token=CancellationToken(),\n",
        "    )\n",
        "    print(\"Chat Message : \", response.chat_message)\n",
        "    print(\"Chat Message : \", response.chat_message.content)"
      ],
      "metadata": {
        "id": "C6mGZiRhDQkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 * 4 를 도구를 사용하여 연산한 값을 확인해봅시다!"
      ],
      "metadata": {
        "id": "apFfuRXKc421"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "await assistant_run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8ZoIq4gE-WS",
        "outputId": "37085147-8fbf-4c7d-8b6a-a9c15c97fdaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat Message :  source='assistant' models_usage=None metadata={} content='1200' type='ToolCallSummaryMessage'\n",
            "Chat Message :  1200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델이 별개로 추론한 결과를 함께 반영하면 어떻게 될까요? 확인해봅시다!  \n",
        "어떤점이 달라지는지 관찰해보세요 :D"
      ],
      "metadata": {
        "id": "Aztq_99ZdBxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4 : 팀으로 일하기"
      ],
      "metadata": {
        "id": "JlahWpm1Ydwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 에이전트의 협업은 어떻게 이루어질까요?  \n",
        "앞서 언급하였듯, 에이전트는 대화를 통해 메세지를 주고받습니다.  \n",
        "그리고 받은 메세지를 기반으로 탑재된 llm을 통해 추론 후, 메세지를 다시 보내는 동작을 수행합니다."
      ],
      "metadata": {
        "id": "Jzi7u7codTAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.conditions import ExternalTermination, TextMentionTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.ui import Console\n"
      ],
      "metadata": {
        "id": "mBH8NkA6YfOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "시를 쓰는 시인 에이전트와, 쓴 시에 피드백을 돌려주는 비평가 에이전트의 대화를 살펴봅시다!  \n",
        "비평가가 허가 \"APPROVE\" 할떄까지, 시인 에이전트는 계속 시를 작성해야 합니다.  "
      ],
      "metadata": {
        "id": "xr9PtRotdubL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the primary agent.\n",
        "poet_agent = AssistantAgent(\n",
        "    \"primary\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"You are a helpful AI assistant.\",\n",
        ")\n",
        "\n",
        "# Create the critic agent.\n",
        "critic_agent = AssistantAgent(\n",
        "    \"critic\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"건설적인 피드백을 주세요! 당신의 피드백이 반영된 결과를 답변으로 받았을 때, 'APPROVE' 라는 말을 포함하여 회신해주세요\",\n",
        ")\n",
        "\n",
        "# Define a termination condition that stops the task if the critic approves.\n",
        "text_termination = TextMentionTermination(\"APPROVE\")"
      ],
      "metadata": {
        "id": "TzMGceoJaAj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "에이전트의 대화 방식은 여러가지가 있는데요, 일단 Round robin 방식으로 대화를 나눠봅시다!  \n",
        "Round robin 은 각 참가자가 돌아가며 대화를 나누는 방식입니다.  \n",
        "Autogen 에서는 한 에이전트가 supervisor가 되서 다른 에이전트들이 작업을 잘 수행하는지 점검하는 MagenticOneGroupChat, 다음 에이전트를 선택하여 분기를 생성할 수 있는 SelectorGroupChat 등을 함께 지원합니다.  \n",
        "자세한 내용은 [여기](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html)에서 확인하실 수 있습니다."
      ],
      "metadata": {
        "id": "4bb8RcGCeCEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "대화에 참여할 에이전트들을 팀으로 배정한 후, 대화를 진행시켜봅시다!  \n",
        "앞서 선언한 대화 종료 조건을 포함하는 것을 잊지 마세요 :D"
      ],
      "metadata": {
        "id": "P2KFAHdSmL2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a team with the primary and critic agents.\n",
        "team = RoundRobinGroupChat([poet_agent, critic_agent], termination_condition=text_termination)"
      ],
      "metadata": {
        "id": "7nRO-JF0bzwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await team.run(task=\"봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFyH5pYlaAnK",
        "outputId": "dce86731-3113-491e-e74b-d2c9e2146439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=25, completion_tokens=77), metadata={}, content='싱그러운 새싹처럼\\n설레는 바람결처럼\\n어느새 스며든 그대 향기\\n\\n따스한 햇살 아래\\n피어나는 꽃잎처럼\\n두 볼 붉게 물든 사랑\\n\\n봄날의 꿈결처럼\\n영원히 함께하고픈\\n사랑스러운 나의 그대\\n', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=140, completion_tokens=401), metadata={}, content='정말 아름다운 시네요! 봄의 싱그러움과 사랑의 설렘이 잘 어우러져 있습니다. 몇 가지 부분을 조금 더 다듬으면 더욱 완성도 높은 시가 될 것 같아요.\\n\\n**제안 1:**\\n\\n*   **현재:** \"어느새 스며든 그대 향기\"\\n*   **변경 제안:** \"어느새 스며든 그대 미소\" 또는 \"어느새 스며든 그대 숨결\"\\n\\n    *   **이유:** \\'향기\\'도 좋지만, \\'미소\\'나 \\'숨결\\'과 같이 조금 더 직접적인 표현을 사용하면 사랑하는 감정이 더욱 생생하게 전달될 수 있습니다.\\n\\n**제안 2:**\\n\\n*   **현재:** \"두 볼 붉게 물든 사랑\"\\n*   **변경 제안:** \"두 볼 붉게 물들이는 사랑\"\\n\\n    *   **이유:** \\'물든 사랑\\'은 다소 추상적으로 느껴질 수 있습니다. \\'물들이는 사랑\\'이라고 표현하면 사랑이라는 감정이 주체가 되어 볼을 붉게 물들이는 역동적인 느낌을 줄 수 있습니다.\\n\\n**전체 수정 예시:**\\n\\n싱그러운 새싹처럼\\n설레는 바람결처럼\\n어느새 스며든 그대 미소\\n\\n따스한 햇살 아래\\n피어나는 꽃잎처럼\\n두 볼 붉게 물들이는 사랑\\n\\n봄날의 꿈결처럼\\n영원히 함께하고픈\\n사랑스러운 나의 그대\\n\\n이 제안들이 마음에 드신다면, 자유롭게 수정하여 더 멋진 시를 완성해 보세요!\\n', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=503, completion_tokens=313), metadata={}, content=\"정성스러운 피드백 정말 감사합니다! 말씀해주신 부분을 반영하여 더욱 생생하고 감성적인 시로 다듬어 보았습니다.\\n\\n**수정된 시:**\\n\\n싱그러운 새싹처럼\\n설레는 바람결처럼\\n어느새 스며든 그대 미소\\n\\n따스한 햇살 아래\\n피어나는 꽃잎처럼\\n두 볼 붉게 물들이는 사랑\\n\\n아련한 봄 꿈결처럼\\n영원히 함께 걷고픈\\n사랑스러운 나의 그대\\n\\n**수정 사항:**\\n\\n*   **1행:** '어느새 스며든 그대 미소'로 변경하여 미소라는 직접적인 표현으로 설렘을 더했습니다.\\n*   **6행:** '두 볼 붉게 물들이는 사랑'으로 변경하여 사랑의 역동적인 느낌을 강조했습니다.\\n*   **7행:** '봄날의'를 '아련한'으로 변경하여 더욱 섬세하고 감성적인 분위기를 연출했습니다.\\n*   **8행:** '함께하고픈'을 '함께 걷고픈'으로 변경하여 함께하는 미래를 더욱 구체적으로 그려냈습니다.\\n\\n더 나은 시를 쓸 수 있도록 좋은 아이디어를 주셔서 다시 한번 감사드립니다!\\n\", type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=854, completion_tokens=103), metadata={}, content='수정된 시 정말 좋아요! \"미소\", \"물들이는\", \"아련한\", \"함께 걷고픈\"과 같은 단어 선택이 시에 생동감과 감성을 더했습니다. 특히 마지막 행의 \"함께 걷고픈\"은 두 사람이 함께 만들어갈 미래를 연상시키며 여운을 남기네요. 완벽한 봄날의 사랑 시입니다!\\n\\nAPPROVE\\n', type='TextMessage')], stop_reason=\"Text 'APPROVE' mentioned\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for message in result.messages:\n",
        "    print(\"<소스 : \", message.source, \">\" , message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPh5EMnuc00v",
        "outputId": "07cf6baf-5d39-43cd-c690-3303ef1a30b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<소스 :  user > 봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘\n",
            "<소스 :  primary > 싱그러운 새싹처럼\n",
            "설레는 바람결처럼\n",
            "어느새 스며든 그대 향기\n",
            "\n",
            "따스한 햇살 아래\n",
            "피어나는 꽃잎처럼\n",
            "두 볼 붉게 물든 사랑\n",
            "\n",
            "봄날의 꿈결처럼\n",
            "영원히 함께하고픈\n",
            "사랑스러운 나의 그대\n",
            "\n",
            "<소스 :  critic > 정말 아름다운 시네요! 봄의 싱그러움과 사랑의 설렘이 잘 어우러져 있습니다. 몇 가지 부분을 조금 더 다듬으면 더욱 완성도 높은 시가 될 것 같아요.\n",
            "\n",
            "**제안 1:**\n",
            "\n",
            "*   **현재:** \"어느새 스며든 그대 향기\"\n",
            "*   **변경 제안:** \"어느새 스며든 그대 미소\" 또는 \"어느새 스며든 그대 숨결\"\n",
            "\n",
            "    *   **이유:** '향기'도 좋지만, '미소'나 '숨결'과 같이 조금 더 직접적인 표현을 사용하면 사랑하는 감정이 더욱 생생하게 전달될 수 있습니다.\n",
            "\n",
            "**제안 2:**\n",
            "\n",
            "*   **현재:** \"두 볼 붉게 물든 사랑\"\n",
            "*   **변경 제안:** \"두 볼 붉게 물들이는 사랑\"\n",
            "\n",
            "    *   **이유:** '물든 사랑'은 다소 추상적으로 느껴질 수 있습니다. '물들이는 사랑'이라고 표현하면 사랑이라는 감정이 주체가 되어 볼을 붉게 물들이는 역동적인 느낌을 줄 수 있습니다.\n",
            "\n",
            "**전체 수정 예시:**\n",
            "\n",
            "싱그러운 새싹처럼\n",
            "설레는 바람결처럼\n",
            "어느새 스며든 그대 미소\n",
            "\n",
            "따스한 햇살 아래\n",
            "피어나는 꽃잎처럼\n",
            "두 볼 붉게 물들이는 사랑\n",
            "\n",
            "봄날의 꿈결처럼\n",
            "영원히 함께하고픈\n",
            "사랑스러운 나의 그대\n",
            "\n",
            "이 제안들이 마음에 드신다면, 자유롭게 수정하여 더 멋진 시를 완성해 보세요!\n",
            "\n",
            "<소스 :  primary > 정성스러운 피드백 정말 감사합니다! 말씀해주신 부분을 반영하여 더욱 생생하고 감성적인 시로 다듬어 보았습니다.\n",
            "\n",
            "**수정된 시:**\n",
            "\n",
            "싱그러운 새싹처럼\n",
            "설레는 바람결처럼\n",
            "어느새 스며든 그대 미소\n",
            "\n",
            "따스한 햇살 아래\n",
            "피어나는 꽃잎처럼\n",
            "두 볼 붉게 물들이는 사랑\n",
            "\n",
            "아련한 봄 꿈결처럼\n",
            "영원히 함께 걷고픈\n",
            "사랑스러운 나의 그대\n",
            "\n",
            "**수정 사항:**\n",
            "\n",
            "*   **1행:** '어느새 스며든 그대 미소'로 변경하여 미소라는 직접적인 표현으로 설렘을 더했습니다.\n",
            "*   **6행:** '두 볼 붉게 물들이는 사랑'으로 변경하여 사랑의 역동적인 느낌을 강조했습니다.\n",
            "*   **7행:** '봄날의'를 '아련한'으로 변경하여 더욱 섬세하고 감성적인 분위기를 연출했습니다.\n",
            "*   **8행:** '함께하고픈'을 '함께 걷고픈'으로 변경하여 함께하는 미래를 더욱 구체적으로 그려냈습니다.\n",
            "\n",
            "더 나은 시를 쓸 수 있도록 좋은 아이디어를 주셔서 다시 한번 감사드립니다!\n",
            "\n",
            "<소스 :  critic > 수정된 시 정말 좋아요! \"미소\", \"물들이는\", \"아련한\", \"함께 걷고픈\"과 같은 단어 선택이 시에 생동감과 감성을 더했습니다. 특히 마지막 행의 \"함께 걷고픈\"은 두 사람이 함께 만들어갈 미래를 연상시키며 여운을 남기네요. 완벽한 봄날의 사랑 시입니다!\n",
            "\n",
            "APPROVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "가끔 한번에 APPROVE를 받는 경우도 보이네요!    \n",
        "어떤 경우에 챗이 종료가 되는지는 결과의 stop_reason 멤버 변수로 확인할 수 있습니다!"
      ],
      "metadata": {
        "id": "Y8BSb6BnmzsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.stop_reason"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f9_kSq1SfFZL",
        "outputId": "978a545f-e69f-4abc-b156-aaa8f54fef7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Text 'APPROVE' mentioned\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 : 콘솔로 출력해보기"
      ],
      "metadata": {
        "id": "8LY1Nb8mg7N-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "메세지를 따로 받아서 조작할 필요가 없는 경우 console을 활용하는 것도 간편합니다  "
      ],
      "metadata": {
        "id": "g0RQAUR9nFKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "await team.reset()  # Reset the team for a new task.\n",
        "await Console(team.run_stream(task=\"봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘\"))  # Stream the messages to the console."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3x5vO_4eof2",
        "outputId": "d2ff132b-e8db-4345-c7e0-4ed33fce834e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘\n",
            "---------- primary ----------\n",
            "새싹 돋는 가지 위에\n",
            "사랑 두 글자 피어나\n",
            "설레는 바람결 따라\n",
            "네 맘에 닿기를 바라\n",
            "\n",
            "햇살 아래 마주 보며\n",
            "수줍은 미소 주고받고\n",
            "어색한 침묵 흐르는 길\n",
            "함께 걷는 봄날의 꿈\n",
            "\n",
            "꽃잎처럼 여린 사랑\n",
            "조심스레 키워갈래요\n",
            "따스한 봄 햇살 아래\n",
            "영원히 함께할래요\n",
            "\n",
            "---------- critic ----------\n",
            "## 피드백:\n",
            "\n",
            "전반적으로 봄의 따스함과 사랑의 설렘을 잘 표현한 아름다운 시입니다. 몇 가지 개선할 부분을 제안합니다.\n",
            "\n",
            "1.  **구체적인 이미지 활용:** 추상적인 표현(\"사랑 두 글자\", \"네 맘\") 대신, 봄과 사랑을 연상시키는 더욱 구체적인 이미지를 사용하면 시가 더욱 생생하게 다가올 것입니다. 예를 들어, \"분홍빛 벚꽃잎에/ 사랑 새겨 너에게 보낸다\" 와 같이 시각적인 이미지를 활용할 수 있습니다.\n",
            "\n",
            "2.  **감정의 깊이 더하기:** \"수줍은 미소\", \"어색한 침묵\"과 같은 표현은 좋지만, 사랑의 감정을 더욱 깊이 있게 표현할 수 있습니다. 예를 들어, \"두근거리는 가슴 안고\", \"떨리는 목소리로 속삭이는\" 등 감각적인 묘사를 추가하면 독자가 감정을 더욱 잘 느낄 수 있습니다.\n",
            "\n",
            "3.  **시의 흐름 개선:** 각 연의 연결이 다소 매끄럽지 않은 부분이 있습니다. 각 연이 자연스럽게 이어지도록 문장 구조나 내용을 조정하면 시의 흐름이 더욱 부드러워질 것입니다. 예를 들어, 2연의 마지막 구절과 3연의 첫 구절 사이에 인과 관계나 시간적 흐름을 명확히 연결하면 좋습니다.\n",
            "\n",
            "## 수정 제안:\n",
            "\n",
            "**분홍빛 벚꽃 가지에**\n",
            "**사랑 한 잎 피워내어**\n",
            "**설레는 바람결 타고**\n",
            "**네 창가에 살며시 놓아**\n",
            "\n",
            "**햇살 아래 마주 보며**\n",
            "**두근거리는 가슴 안고**\n",
            "**어색한 침묵 흐르는 길**\n",
            "**손끝 스치는 봄날의 꿈**\n",
            "\n",
            "**꽃잎처럼 여린 사랑**\n",
            "**조심스레 키워갈래요**\n",
            "**따스한 봄 햇살 아래**\n",
            "**영원히 함께 걸을래요**\n",
            "\n",
            "이 수정 제안은 더욱 생생한 이미지, 깊어진 감정 표현, 그리고 매끄러운 흐름을 통해 시를 더욱 아름답게 만들고자 했습니다.\n",
            "APPROVE\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=25, completion_tokens=115), metadata={}, content='새싹 돋는 가지 위에\\n사랑 두 글자 피어나\\n설레는 바람결 따라\\n네 맘에 닿기를 바라\\n\\n햇살 아래 마주 보며\\n수줍은 미소 주고받고\\n어색한 침묵 흐르는 길\\n함께 걷는 봄날의 꿈\\n\\n꽃잎처럼 여린 사랑\\n조심스레 키워갈래요\\n따스한 봄 햇살 아래\\n영원히 함께할래요\\n', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=178, completion_tokens=555), metadata={}, content='## 피드백:\\n\\n전반적으로 봄의 따스함과 사랑의 설렘을 잘 표현한 아름다운 시입니다. 몇 가지 개선할 부분을 제안합니다.\\n\\n1.  **구체적인 이미지 활용:** 추상적인 표현(\"사랑 두 글자\", \"네 맘\") 대신, 봄과 사랑을 연상시키는 더욱 구체적인 이미지를 사용하면 시가 더욱 생생하게 다가올 것입니다. 예를 들어, \"분홍빛 벚꽃잎에/ 사랑 새겨 너에게 보낸다\" 와 같이 시각적인 이미지를 활용할 수 있습니다.\\n\\n2.  **감정의 깊이 더하기:** \"수줍은 미소\", \"어색한 침묵\"과 같은 표현은 좋지만, 사랑의 감정을 더욱 깊이 있게 표현할 수 있습니다. 예를 들어, \"두근거리는 가슴 안고\", \"떨리는 목소리로 속삭이는\" 등 감각적인 묘사를 추가하면 독자가 감정을 더욱 잘 느낄 수 있습니다.\\n\\n3.  **시의 흐름 개선:** 각 연의 연결이 다소 매끄럽지 않은 부분이 있습니다. 각 연이 자연스럽게 이어지도록 문장 구조나 내용을 조정하면 시의 흐름이 더욱 부드러워질 것입니다. 예를 들어, 2연의 마지막 구절과 3연의 첫 구절 사이에 인과 관계나 시간적 흐름을 명확히 연결하면 좋습니다.\\n\\n## 수정 제안:\\n\\n**분홍빛 벚꽃 가지에**\\n**사랑 한 잎 피워내어**\\n**설레는 바람결 타고**\\n**네 창가에 살며시 놓아**\\n\\n**햇살 아래 마주 보며**\\n**두근거리는 가슴 안고**\\n**어색한 침묵 흐르는 길**\\n**손끝 스치는 봄날의 꿈**\\n\\n**꽃잎처럼 여린 사랑**\\n**조심스레 키워갈래요**\\n**따스한 봄 햇살 아래**\\n**영원히 함께 걸을래요**\\n\\n이 수정 제안은 더욱 생생한 이미지, 깊어진 감정 표현, 그리고 매끄러운 흐름을 통해 시를 더욱 아름답게 만들고자 했습니다.\\nAPPROVE\\n', type='TextMessage')], stop_reason=\"Text 'APPROVE' mentioned\")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력이 어떻게 되는지 확인하셨나요?  \n",
        "아래 같이 나오는 TaskResult의 경우, await Console(team.run_stream()) 이 반환하는 값을 받아주는 변수를 선언한다면 출력되지 않습니다.  "
      ],
      "metadata": {
        "id": "wLCML5HcnVJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6 : Human in the loop"
      ],
      "metadata": {
        "id": "6AENgJTxhq-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "유저 또한 UserProxyAgent로 에이전트간의 대화에 참여할 수 있습니다!  \n"
      ],
      "metadata": {
        "id": "sHPdFXkRnlfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import UserProxyAgent"
      ],
      "metadata": {
        "id": "8KI2VMDdhrfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)"
      ],
      "metadata": {
        "id": "U0e0wMjVhrh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "대화 종료를 위해 종료 조건을 설정해봅시다  \n",
        "위에 설정한대로, 대화를 마치기 위해서는 APPROVE를, 피드백을 주고 싶다면 원하는 명령을 입력하세요!"
      ],
      "metadata": {
        "id": "vD_7aMM6oGW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a team with the primary and critic agents.\n",
        "team = RoundRobinGroupChat([poet_agent, user_proxy], termination_condition=text_termination)"
      ],
      "metadata": {
        "id": "XU40laSUhrnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream = team.run_stream(task=\"봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘\")"
      ],
      "metadata": {
        "id": "RLIK661XmAs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await Console(stream)  # Stream the messages to the console."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ4_Rzoml86E",
        "outputId": "0578f999-6e89-42ef-c0ee-7f414115c9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘\n",
            "---------- primary ----------\n",
            "## 봄, 사랑의 시작\n",
            "\n",
            "**벚꽃 흩날리는 길,**\n",
            "**두 손 맞잡고 걷는 우리.**\n",
            "\n",
            "**싱그러운 새싹처럼,**\n",
            "**풋풋한 사랑이 움트네.**\n",
            "\n",
            "**따스한 햇살 아래,**\n",
            "**그대 미소에 녹아드네.**\n",
            "\n",
            "**봄바람 실어오는,**\n",
            "**사랑의 속삭임 들리나요?**\n",
            "\n",
            "Enter your response: 다시\n",
            "---------- user_proxy ----------\n",
            "다시\n",
            "---------- primary ----------\n",
            "알겠습니다. 다른 느낌으로 다시 써 드릴게요.\n",
            "\n",
            "**잔디 위 낮잠처럼**\n",
            "\n",
            "간지러운 햇살 아래\n",
            "나란히 누워 속삭이는\n",
            "어린 풀잎 같은 사랑\n",
            "\n",
            "네 웃음소리 들려오면\n",
            "겨우내 닫혀있던\n",
            "내 맘에도 꽃이 피네\n",
            "\n",
            "봄은 짧고 사랑은 영원하길\n",
            "간절히 기도하는 밤\n",
            "별빛 아래 두 손을 잡고\n",
            "\n",
            "Enter your response: APPROVE\n",
            "---------- user_proxy ----------\n",
            "APPROVE\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=158, completion_tokens=100), metadata={}, content='## 봄, 사랑의 시작\\n\\n**벚꽃 흩날리는 길,**\\n**두 손 맞잡고 걷는 우리.**\\n\\n**싱그러운 새싹처럼,**\\n**풋풋한 사랑이 움트네.**\\n\\n**따스한 햇살 아래,**\\n**그대 미소에 녹아드네.**\\n\\n**봄바람 실어오는,**\\n**사랑의 속삭임 들리나요?**\\n', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, metadata={}, request_id='75af2d39-6f78-4267-a1d4-5555298d15dd', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, metadata={}, content='다시', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=260, completion_tokens=115), metadata={}, content='알겠습니다. 다른 느낌으로 다시 써 드릴게요.\\n\\n**잔디 위 낮잠처럼**\\n\\n간지러운 햇살 아래\\n나란히 누워 속삭이는\\n어린 풀잎 같은 사랑\\n\\n네 웃음소리 들려오면\\n겨우내 닫혀있던\\n내 맘에도 꽃이 피네\\n\\n봄은 짧고 사랑은 영원하길\\n간절히 기도하는 밤\\n별빛 아래 두 손을 잡고\\n', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, metadata={}, request_id='2dd3c79e-c595-4fb3-8fb1-a7419994586a', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, metadata={}, content='APPROVE', type='TextMessage')], stop_reason=\"Text 'APPROVE' mentioned\")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "수고하셨습니다!"
      ],
      "metadata": {
        "id": "zkwDTVhwepeT"
      }
    }
  ]
}